[{"content":"Innerhalb des Seminars haben wir uns als Gruppe zu jedem Schritt des Lebenszyklus von multimedialen Daten Gedanken gemacht und recherchiert wie dieser Schritt innerhalb unseres Medientyp Video abläuft bzw. welche Vorgehensweisen es gibt. Gut fand ich den Hinweis von Herrn Dr. Arndt die Betrachtung auf ein spezielles Einsatzgebiet dieses Medientypens einzuschränken. Hierbei haben wir uns für die professionelle Videoproduktion entschieden. Diese Beschränkung hatte, reflektiert betrachtet, einige Vorteile, sodass das Themengebiet für die Recherche der einzelnen Aufgaben klar abgegrenzt war und nicht jedes Gruppenmitglied willkürlich etwas zu dem breiten Feld Video geforscht hat.\nDie Idee bzw. das Konzept sich innerhalb der Seminare in einer Gruppe mit den einzelnen Fragestellungen zu beschäftigen und auszutauschen bewerte ich insgesamt als gelungen, da so eine weitere Sicht (ausgehend von der Reflexion der Vorlesung) auf das Themengebiet erlangt werden konnte. Auch die praktischen Anwendungen zur direkten Themen der Vorlesung, wie beispielsweise die Auseinandersetzung mit der RDFLib in Python und das Erstellen einiger RDF Triples innerhalb des Seminars haben mir Spaß gemacht und das erlernte Wissen vertieft. Hier sei anzumkeren, dass ich eventuell auch die Behandlung von OpenRefine auf eine Vorlesung und eventuell eine Hälfte eines Seminars aufteilen würde, da aufgrund der komplexeren Installation innerhalb der Vorlesung keine “intensive” Auseinandersetzung mit dem Tool möglich war.\nMir persönlich hat die zeitliche Gestaltungen innerhalb der Seminare nicht immer gefallen. Ich hatte oft das Gefühl, dass entweder die Aufgabenstellungen so offen formuliert waren, dass wir innerhalb unserer Gruppe nicht wussten, was wir genau recherchieren oder dokumentieren sollten oder sie war so konkret formuliert, dass wir nach der Hälfte der Zeit bereits fertig waren. Dies kann möglicherweise auch daran gelegen haben, dass wir oft nur zu dritt innerhalb der Gruppe waren und somit weiterer Input gefehlt hat. Aufgrund der Situation, dass wir oft schon früher fertig waren, haben wir dann die Zeit mit Warten verbracht. Diese Zeit hätte man definitiv besser und effektiver verbringen können, weshalb man sich auch hier selbst kritisieren muss. Ebenso hatte ich das Gefühl, dass die anderen Gruppen diese Problematik nicht hatten. Daraus resultiert auch noch ein weiterer negativer Punkt: Für die Besprechung der Ergebnisse der Gruppenarbeit blieb oft nur noch wenig Zeit, sodass die Gruppen die Themen nur dürftig vorstellen konnten aus Angst die Zeit zu sprengen (5 min je Gruppe).\nDennoch fand ich die Seminar gelungen, interaktiv und bereichernd. Auch hier konnte ich einiges lernen.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/reflexion-seminar/","summary":"Innerhalb des Seminars haben wir uns als Gruppe zu jedem Schritt des Lebenszyklus von multimedialen Daten Gedanken gemacht und recherchiert wie dieser Schritt innerhalb unseres Medientyp Video abläuft bzw. welche Vorgehensweisen es gibt. Gut fand ich den Hinweis von Herrn Dr. Arndt die Betrachtung auf ein spezielles Einsatzgebiet dieses Medientypens einzuschränken. Hierbei haben wir uns für die professionelle Videoproduktion entschieden. Diese Beschränkung hatte, reflektiert betrachtet, einige Vorteile, sodass das Themengebiet für die Recherche der einzelnen Aufgaben klar abgegrenzt war und nicht jedes Gruppenmitglied willkürlich etwas zu dem breiten Feld Video geforscht hat.","title":"Reflexion Seminar"},{"content":"Innerhalb dieses Moduls habe ich viele Konzepte und Techniken des Semantic Webs kennengelernt und mich intensiv mit der Beschreibung von Ressourcen mittels RDF beschäftigt. Die Abwechslung zwischen theoretischen und praktischen Herangehensweisen an verschiedene Themen des Semantik Webs fand ich persönlich gut und gelungen. Ich habe beispielsweise die typische Vorgehensweise bei der Erstellung von Open Linked bzw. FAIR Data mittels RDF kennengelernt und bin der Zuversicht diese Prinzipien in eigenen praktischen Projekten/Veröffentlichungen umzusetzen zu können. Ebenso habe ich mit Ontologien und deren Definition mithilfe von Technologien beschäftigt.\nReflektierend betrachtet und nach Überdenken von Diskussionen sowie nach lesen von Einträgen innerhalb dieses Portfolios ist mir bewusst geworden, dass die Inhalte der Vorlesung starken technischen Bezug haben und sich meines Erachtens nach stark auf Semantic Web ausrichten. Dies spiegelt sich ebenfalls in meiner Mindmap wieder. Persönlich heiße ich diese Ausrichtung gut, da mich der technische Hintergrund oft mehr interessiert als eine rein konzeptuelle Betrachtungsweise. Ich kann auch die Beweggründe für die Gastvorträge, die meist eher eine konzeptuelle bzw. High-Level Sicht auf das Themengebiet Multi Media Lifecycle bieten, nachvollziehen, obwohl ich in den einzelnen Beiträgen oft “kritisiert” habe, dass diese wenigen bzw. keinen starken direkten Bezug zur Vorlesung hatten. Ich denke, dass dieses Empfinden daraus resultiert, dass die die Vorlesungsthemen sowie die Exkursthemen unterschiedliche Sichtweisen auf das Themengebiet haben und somit nicht “direkt” und für mich offensichtlich verknüpfbar waren. Besonders gelungen fand ich die didaktischen Herangehensweisen von Herrn Dr. Arndt. Der regelmäßige Einsatz des Umfragetools in Big Blue Button innerhalb der Vorlesungen empfand ich als sinnvoll und bat mir die Möglichkeit direkten Feedback an Herr Dr. Arndt zu geben, sowohl als auch meine Erwartungen mit den anderen Studierenden abzugleichen. Auch die “besondere” Prüfungsleistung im Sinne dieses Lernportfolio finde ich gut und verkörpert neue und mir bis dato unbekannte Methoden.\nIm Ganzen betrachtet fand ich das Modul Media Lifecycle Management und die Vorlesungen als sehr gut und es hat mir Spaß gemacht an diesen teilzunehmen. Ich denke auch, dass ich einiges gelernt habe und mithilfe dieses Portfolios verschiedene Konzepte in Zukunft noch einmal nachlesen kann.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/reflexion-vorlesung/","summary":"Innerhalb dieses Moduls habe ich viele Konzepte und Techniken des Semantic Webs kennengelernt und mich intensiv mit der Beschreibung von Ressourcen mittels RDF beschäftigt. Die Abwechslung zwischen theoretischen und praktischen Herangehensweisen an verschiedene Themen des Semantik Webs fand ich persönlich gut und gelungen. Ich habe beispielsweise die typische Vorgehensweise bei der Erstellung von Open Linked bzw. FAIR Data mittels RDF kennengelernt und bin der Zuversicht diese Prinzipien in eigenen praktischen Projekten/Veröffentlichungen umzusetzen zu können.","title":"Reflexion Vorlesung"},{"content":"   Nummer Fachbegriff Definition     1 Media Lifecycle Management Verwaltung des Lebenszyklus von Daten, insbesondere medialen Daten. Suche/Erkunden → Extraktion → Speicherung → Überarbeitung → Kopplung → Klassifikation → Qualitätsanalyse → Reparatur/Weiterentwicklung   2 Video Ein kontinuierliches (zeitabhängiges) Medium, das die Medien Bild und Audio kombiniert. Die bildliche Komponente spiegelt sich als Folge von Bildern wieder, die mit hinreichender Geschwindigkeit abgespielt werden, sodass das menschliche Auge nicht in der Lage ist, die einzelnen Bilder zu trennen.   3 Web of Data Das Web 2.0 wird so erweitert, dass Informationen auf Webseiten als semantisch strukturierte Inhalte vorliegen und eine Verlinkung zwischen den Informationen möglich ist. Dafür müssen Informationen insbesondere maschinenlesbar vorliegen und verlinkt werden können.   4 Resource Description Framework Eine wichtige fundamentale Technik zur Realisierung des Web of Data. Mithilfe von RDF können Aussagen zu Ressourcen definiert werden. Diese werden in Form von einem Triple gespeichert und kann als gerichteter Graph repräsentiert werden.   5 Linked Open Data Beschreibt frei verfügbare Daten, die per URI identifizierbar sind und Verknüpfungen auf andere Ressourcen besitzen. Für die Kodierung der Daten kann RDF verwendet werden.   6 Ontologie Ontologien repräsentieren ein Schema zur Darstellung einer Menge von Begriffen und bestehender Beziehungen in einem definierten Bezugsrahmen. Ermöglichen aufgrund von Inferenz- und Integritätsregeln die Integration und Schlussfolgerung von Wissen. Weitere Definition: [\u0026hellip;] ist eine formale und explizite Spezifikation einer gemeinsamen Konzeptionierung. Sie zeichnet sich durch eine hohe semantische Ausdruckskraft, die für Beschreibungen mit erhöhter Komplexität benötigt wird.   7 Uniform Resource Identifier Dient als Identifikator einer Ressourcen, der in Form einer schematischen Zeichenkette repräsentiert wird. Eine URI besteht aus fünf Bestandteilen: scheme, authority, path, query, fragment.   8 URI/IRI URI steht für Uniform Resource Identifier und IRI für Internationalized Resource Identifier und dienen im Kontext RDF als eindeutige Bezeichner für Ressourcen. IRIs sind dabei eine internationalisierte Form der URIs, indem Unicode-Zeichen innerhalb des Identifikators erlaubt sind.   9 RDF-Triple RDF Triple bestehen aus einem Subjekt, einem Prädikat und einem Objekt. Zusammen bilden diese Wörter eine logische Aussage über die Ressource. Sie sind die Basis aller RDF-Technologien.   10 RDF Schema Bietet Möglichkeiten Eigenschaften und Relationen einer Ressource zu modellieren und diese Einzuschränken. Dabei werden zur Beschreibung ebenfalls RDF Triples verwendet. Alternativen zu RDF Schema sind OWL und SHACL.   12 RDF-Stack Ein anderes Wort für RDF-Stack ist der Semantik Web Stack. Dieser illustriert die wesentliche Architektur von RDF/des Semantik Webs und beinhaltet viele Prinzipien und Technologien, die in Zusammenhang mit RDF verwendet werden.   13 Serialisierung von RDF-Graphen Mithilfe der Serialisierung und verschiedene Serialisierungsformate können RDF-Graphen in Form von RDF-Triples syntaktisch und semantisch korrekt notiert und somit serialisiert werden. Aufgrund dieser Verwendung werden Serialsierungsformate innerhalb der Abstrakten Language im RDF-Stack verortet.   14 Turtle Turtle ist eines von vielen Serialsierungsformate für RDF. Es setzt dabei den Schwerpunkt auf Lesbarkeit für den Menschen. Repetitionen von Subjekten und Prädikaten können durch eine einfache Syntax vermieden werden.   15 DBpedia Ein Projekt, welches semi-strukturierte Daten aus Wikipedia extrahiert und aus diesen RDF-Daten extrahiert. Ziel ist es für Abfragen bessere Ergebnisse und aggregiertes Wissen zu liefern.   16 Wikidata Wikidata ist eine Wissensdatenbank, die frei und öffentlich ist. Sie kann sowohl von Menschen als auch von Maschinen bearbeitet werden und dient als zentraler Speicher für beispielsweise Wikipedia.   17 SPARQL SPARQL ist eine graphenbasierte Abfragesprache und dient dazu Queries an RDF-Graphen zu senden.   18 OpenRefine Mithilfe der RDF-Extension für OpenRefine kann die Software für die Bereinigung und Transformation der Daten in RDF Triple genutzt werden. Dafür werden die Daten in einer Spalte entsprechend einem Objekt zugeordnet (Mapping). Für die Bereinigung stehen zahlreiche built-in Funktionen zur Verfügung.   19 A-Box und T-Box Die A- und T-Box kategorisieren verschiedene Statements in einem Wissensgraphen. Die A-Box enthält Wissen, die die eigentliche Ressource betreffen, wohingegen die T-Box das Schema- und Domänenwissen enthält. Innerhalb der T-Box werden die Klassen und dessen Eigenschaften beschrieben.   20 Reifikation Innerhalb des RDF Schemas wird diesen Term für die Verdinglichung eines Statements verwendet. Dies erlaubt den Nutzer, Aussagen über eine Aussage, also ein RDF-Triple selbst, zu treffen.   21 Web Ontology Language OWL ist eine Spezifikation des W3C und dient zur Erstellung von Ontologien. OWL(Full) basiert dabei auf RDF Schema und erweitert einige Konzepte. OWL Dokumente bestehen dabei aus Axiomen, welche ein oder mehrere RDF-Triples sind.   22 Shape Constraint Language SHACL ist eine weitere Spezifikation des W3C und dient zur Validierung von RDF Graphen anhand einer Menge von Bedingungen. Diese Bedingungen werden über Shapses und Constraints definiert. SHACL wird dabei ebenfalls per RDF Triples ausgedrückt.   23 Five Star Open Data Five Star Open Data ist ein 5-Sterne-Modell für offene Daten. Dieses Modell wurde von Tim Berners-Lee vorgeschlagen und kategorisiert veröffentlichte Daten nach ihrer Offenheit und Benutzbarkeit/Verbreitbarkeit. Das Modell ist dabei kumulativ, d. h. das für eine höhere Stufe die vorangegangenen erfüllt sein müssen.   24 Netzwerk-Effekt Ein Effekt, bei dem der Wert/Nutzen mit steigender Nutzerzahl ebenfalls steigt. Innerhalb des Kontextes RDF kann dies bei Verlinkungen der Fall sein. Je mehr Herausgeber die Daten verlinkbar machen und auf andere Daten verlinken, desto besser kann Wissen aggregiert und zugänglich gemacht werden.   25 Content Negotiation Ist ein Mechanismus innerhalb der HTTP Spezifikation um verschiedene Versionen eines Dokumentes für die gleiche URI auszuliefern. Versionen meinen dabei Beispielsweise eine RDF- oder eine HTML-Datei, zwischen der vor der Auslieferung entschieden wird.   26 FAIR Data Das Akronym FAIR steht für Findable, Accessible, Interoperable und Reusable. Daten werden als FAIR bezeichnet, wenn sie all diese vier Grundsätzen entsprechen.   27 Virtual Research Organization Hierbei handelt es sich um eine Organisation bestehend aus verschiedenen Mitgliedern, die zu unterschiedlichen Unternehmen oder akademischen Forschungseinheiten gehören. Diese Mitglieder arbeiten an einer Technologie/Ziel zusammen. Für diese Organisation gelten die verschiedenen Dimensionen der verteilten Zusammenarbeit.   28 OntoWiki Es handelt sich um eine semantische Wiki-Anwendung, welche Open-Source ist. Die Software dient als Ontologie-Editor oder als Wissenserfassungssysten.   28 Datenmanagementplan Ist ein formelles Dokument, welches den Umgang mit Forschungsdaten strukturiert und beschreibt, wie während als auch nach einem Forschungsprojekt die Daten behandelt wurden und werden sollen. Er enthält unter anderem eine Projekt- sowie Datenbeschreibung, Aspekte zur Qualität und Speicherung, sowohl auch rechtliche Aspekte.    ","permalink":"https://fredericbahr.github.io/lernportfolio/posts/glossar/","summary":"Glossar für die Begriffe: Media Lifecycle Management, Video, Web of Data, Resource Description Framework, Linked Open Data, Ontologie, Uniform Resource Identifier, Interantionalized Resource Identifier, RDF-Triple, RDF Schema, RDF Stack, Serialisierung von RDF-Graphen, Turtle, DBpedia, Wikidata, SPARQL, OpenRefine, A-Box und T-Box, Reifikation, Web Ontology Language, Shape Constraint Language, Five Star Open Data, Netzwerk-Effekt, Content Negotiation, FAIR Data, Virtual Research Organization, OntoWiki, Datenmanagementplan.","title":"Glossar"},{"content":"Per Klick auf das Bild kann dieses größer betrachtet und heruntergeladen werden.\n\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/mindmap/","summary":"Mindmap zu den Begriffen definiert im Glossar","title":"Mindmap"},{"content":"","permalink":"https://fredericbahr.github.io/lernportfolio/posts/vorlesung/vorlesung-13/","summary":"","title":"Vorlesung 13"},{"content":"","permalink":"https://fredericbahr.github.io/lernportfolio/posts/seminar/seminar-12/","summary":"","title":"Seminar 12"},{"content":"Exkurs Deutsche Digitale Bibliothek:\nExkurs eccenca GmbH: Die eccenca Gmbh ist ein europaweit agierendes Unternehmen mit Start-Up-DNA, die durch eine Ausgliederung aus einer Forschungsgruppe entstand. Ziel ist es die Digitalisierung in Unternehmen zu beschleunigen und zu verbessern. Dabei wird auf eine Vermarktung von Wissensgraphen gesetzt. Um die Digitalisierung zu erreichen sollen Daten aus sogenannten Datensilos extrahiert und migriert/integriert werden. Dabei wird auf einen sogenannten Business Digital Twin gesetzt. Überraschend für mich war der Iterative Prozess in dem zuerst die Daten analysiert und ein Domänenwissen aufgebaut wird, mithilfe dieses Wissens wird das semantische Modell, bestehend aus Ontologien und Vokabularen, aufgebaut. Mittels der Ontologien kann dann ein Mapping zwischen Daten und Ontology stattfinden und die Daten verlinkt, angereichert und bereinigt werden. Nach Reviews mit dem Kunden wird dieser Prozess wiederholt und die einzelnen Artefakte verbessert.\nIch fande diesen Vortrag von den drei Exkursen am besten, da viele Themen der Vorlesung aufgegriffen und Assoziationen zwischen der Theorie und der Praxis hergestellt wurden. Beispielsweise führt die eccenca ein ähnlichen Prozess durch, den wir mit OpenRefine verwendet haben, um Daten zu mappen und zu bereinigen. Ebenfalls beeindruckend fand ich den unterschiedlichen Tech-Stack der jeweiligen Applikationen: Java und Scala auf der einen Seite, Python und JavaScript (React) auf der anderen. Nicht zu vergessen die Technologien für RDF (SPARQL, Turtle, Triple Stores). Dies verdeutlichte noch einmal wie breit gefächert das Gebiet des Semantik Webs in der Praxis ist.\nWünschenswert wäre mehr Zeit für die Demo der Software selbst und weniger marketingtechnische Ausführungen zu essenca selbst, da (mich persönlich innerhalb einer Vorlesung) die technischen Belange mehr interessieren.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/vorlesung/vorlesung-12/","summary":"Exkurs Deutsche Digitale Bibliothek:\nExkurs eccenca GmbH: Die eccenca Gmbh ist ein europaweit agierendes Unternehmen mit Start-Up-DNA, die durch eine Ausgliederung aus einer Forschungsgruppe entstand. Ziel ist es die Digitalisierung in Unternehmen zu beschleunigen und zu verbessern. Dabei wird auf eine Vermarktung von Wissensgraphen gesetzt. Um die Digitalisierung zu erreichen sollen Daten aus sogenannten Datensilos extrahiert und migriert/integriert werden. Dabei wird auf einen sogenannten Business Digital Twin gesetzt. Überraschend für mich war der Iterative Prozess in dem zuerst die Daten analysiert und ein Domänenwissen aufgebaut wird, mithilfe dieses Wissens wird das semantische Modell, bestehend aus Ontologien und Vokabularen, aufgebaut.","title":"Vorlesung 12"},{"content":"Das Seminar wurde nach Rücksprache mit uns Studierenden in zwei thematische Abschnitte unterteilt. Ich persönlich bewerte dieses Vorgehen als sehr gut, da Herr Dr. Arndt auf die Bedürfnisse und Wünsche der Studierenden eingegangen ist und Rücksicht (besonders auf für mich relevante Gebiete) genommen hat. Neben einer kurzen Einführung der Software OntoWiki, in der Herr Dr. Arndt erklärte, wie Klassen, Properties und Instanzen richtig angelegt werden, haben wir in unserer Gruppe einige Objekte modelliert. Die praktische Auseinandersetzung mit der Thematik und der Software hat für mich persönlich das Wissen, gerade über den Aufbau von RDF und Ontologien, gefestigt. Ebenso konnten so auch Einblicke in die Praxis erlangt werden.\nEbenfalls habe ich zusammen mit meiner Gruppe versucht die Frage “Wie kann ich Qualität messen” in Bezug auf professionell produzierte Videos zu beantworten. In Anbetracht der Zeit konnten lediglich die wichtigsten Qualitätsmetriken und Merkmale betrachtet werden. Dennoch empfand ich das Seminar als sehr positiv. Mir hat vor allem die Arbeit mit der Software OntoWiki Spaß gemacht.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/seminar/seminar-11/","summary":"Das Seminar wurde nach Rücksprache mit uns Studierenden in zwei thematische Abschnitte unterteilt. Ich persönlich bewerte dieses Vorgehen als sehr gut, da Herr Dr. Arndt auf die Bedürfnisse und Wünsche der Studierenden eingegangen ist und Rücksicht (besonders auf für mich relevante Gebiete) genommen hat. Neben einer kurzen Einführung der Software OntoWiki, in der Herr Dr. Arndt erklärte, wie Klassen, Properties und Instanzen richtig angelegt werden, haben wir in unserer Gruppe einige Objekte modelliert.","title":"Seminar 11"},{"content":"  div .quizQuestion { margin: 2rem 0; position: relative; border-radius: .2rem; border: 1px solid var(--border); color: var(--primary); padding: .5rem 1rem .5rem 2rem; } div.quizQuestion p { padding: 0px; display: block; font-size: 1rem; margin-top: 0rem; margin-bottom: 0rem; } div.quizQuestion p:first-child:before { position: absolute; top: -27px; color: var(--primary); font-family: FontAwesome; left: 10px; } div.quizQuestion p:first-child:after { position: absolute; top: -27px; color: var(--primary); left: 2rem; } div.quizQuestion p:first-child:after { color: var(--primary); content: 'Quiz-Frage'; } div.quizQuestion { border-top: 30px solid var(--border); background: var(--theme); color: var(--primary) !important; }  Quizfrage: Wofür steht das Akronym FAIR und was sind die Kernelemente der einzelnen Abkürzungen? Antwort: FAIR steht für die Wörter Findable, Accessible, Interoperable und Reusable. Findable bedeutet, dass Daten mit einem eindeutigen Identifikator und umfangreichen Metadaten versehen sind. Accessible verdeutlicht, dass die Daten mittels anerkannter und standardisierten Kommunikationsprotokollen abgerufen werden können. Interoperable meint dabei, dass Daten ausgetauscht, interpretiert und mit anderen Datensätzen kombiniert werden können. Zuletzt sollten die Daten aufgrund ihrer Lizenz und Dokumentation für eine Nachnutzung geeignet sein.\n  div .answeredQuestion { margin: 2rem 0; position: relative; border-radius: .2rem; border: 1px solid var(--border); color: var(--primary); padding: .5rem 1rem .5rem 2rem; } div.answeredQuestion p { padding: 0px; display: block; font-size: 1rem; margin-top: 0rem; margin-bottom: 0rem; } div.answeredQuestion p:first-child:before { position: absolute; top: -27px; color: var(--primary); font-family: FontAwesome; left: 10px; } div.answeredQuestion p:first-child:after { position: absolute; top: -27px; color: var(--primary); left: 2rem; } div.answeredQuestion p:first-child:after { color: var(--primary); content: 'Beantwortete Frage(n)'; } div.answeredQuestion { border-top: 30px solid var(--border); background: var(--theme); color: var(--primary) !important; }  Frage: Wieso benötigen wir strukturierte Datenformate im Web?\nAntwort: Strukturierte Datenformate sind maschinenlesbar und erlauben eine automatisierte Verarbeitung. Dafür ebenfalls wichtig ist eine Standardisierung der Datenformate. Ebenfalls können damit gut komplexe Daten dargestellt werden. Die Daten sollten ausgetauscht, interpretiert und automatisiert verknüpft werden können.\n Frage: Wie können wir dafür sorgen, dass unsere Daten verlinkbar sind, warum sollten wir auf andere Daten verlinken?\nAntwort: Durch die Verwendung von URIs bzw. IRIs ermöglicht man die Verlinkung auf seine Ressourcen. Verlinkt man ebenfalls auf andere Daten, so können Nutzer einen Kontext zu seinen Informationen herstellen und durch die Verlinkung trägt man zu dem sogenannten Netzwerkeffekt bei. Ebenfalls kann durch die Verlinkung verwandtes Wissen aggregiert werden.\n In der Vorlesungen haben wir uns zunächst mit den FAIR-Prinzipien für Daten beschäftigt. Anschließend haben wir uns angeschaut, mit welchen Hilfsmitteln Daten klassifiziert und angereichert werden können. Dabei fand ich erstaunlich, dass eine weitere Klassifikation meist auch eine Anreicherung darstellt. Dieser Blickwinkel war für mich neu und hat mich überrascht. Ebenso empfand ich die kleinen Demobeispiele einiger Tools als gut, da dies die Vorlesung aufgelockerte und praktische Aspekte vermittelte. Weiterhin haben wir uns mit der Datenqualität beschäftigt, was einerseits eine schlechte Qualität für Auswirkungen hat und welche Metriken es für die Datenqualität gibt. Spannend fand ich die Erläuterungen zu Teilen der Doktorandenarbeit von Herrn Arndt über die Herausforderungen und Möglichkeiten der verteilten Zusammenarbeit.\nLeider hatte ich das Gefühl, dass die Vorlesung/Themen etwas unsortiert waren und wir durch mehrere Themengebiete gesprungen sind. Dies reduzierte meine Auffassungsgabe, da ich mich stets in etwas neues hineindenken musste. Dadurch konnte ich mir kein visuelles Mindmap erstellen, sodass ich die Themen schwer sortieren konnte.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/vorlesung/vorlesung-11/","summary":"In der Vorlesungen haben wir uns zunächst mit den FAIR-Prinzipien für Daten beschäftigt. Anschließend haben wir uns angeschaut, mit welchen Hilfsmitteln Daten klassifiziert und angereichert werden können. Dabei fand ich erstaunlich, dass eine weitere Klassifikation meist auch eine Anreicherung darstellt. Dieser Blickwinkel war für mich neu und hat mich überrascht. Ebenso empfand ich die kleinen Demobeispiele einiger Tools als gut, da dies die Vorlesung aufgelockerte und praktische Aspekte vermittelte. Weiterhin haben wir uns mit der Datenqualität beschäftigt, was einerseits eine schlechte Qualität für Auswirkungen hat und welche Metriken es für die Datenqualität gibt. Spannend fand ich die Erläuterungen zu Teilen der Doktorandenarbeit von Herrn Arndt über die Herausforderungen und Möglichkeiten der verteilten Zusammenarbeit.\nLeider hatte ich das Gefühl, dass die Vorlesung/Themen etwas unsortiert waren und wir durch mehrere Themengebiete gesprungen sind. Dies reduzierte meine Auffassungsgabe, da ich mich stets in etwas neues hineindenken musste. Dadurch konnte ich mir kein visuelles Mindmap erstellen, sodass ich die Themen schwer sortieren konnte.","title":"Vorlesung 11"},{"content":"Innerhalb des Seminars hat sich unsere Gruppe mit der Gruppierungs-, Einteilung- sowie der Kategorisierungsmöglichkeiten von professionell produzierten Videos beschäftigt. Ebenfalls haben wir betrachtet, wie Metadaten angereichert werden können um zusätzliche Informationen zu codieren. Neben den klassischen Gruppierungsmöglichkeiten nach Genre oder Stilmittel haben wir uns auch damit beschäftigt, wie Filme nach dem Setting oder dem narrativen Stil kategorisiert werden können. Zusätzlich haben wir Einteilungsmöglichkeiten wie Altersfreigabe und Regisseure oder andere Metadaten gefunden. Interessant war die Recherche zu einer Software, die die einzelnen Frames eines Videos analysiert und eine aggregierte Visualisierung zu verschiedenen Bildmetriken des einzelnen Frames darstellt.\nDie Untersuchung der Arten der Anreicherung ist uns zunächst schwer gefallen, jedoch konnte mit Hilfe von Herr Dr. Arndt verschiedene Techniken gefunden werden: Beispielsweise können Graphdatenbanken, wie imdb.de das realisiert hat, verwendet oder RDF-Graphen entwickelt werden. Zusätzlich können die Metadatenformate verändert und erweitert werden.\nEtwas störend empfand ich erneut die viele Zeit, die uns zur Verfügung stand. Nach knapp einer Stunde war unsere Gruppe (auch aufgrund von lediglich drei Personen) mit der Recherche/Diskussion fertig, sodass wir warten mussten, bis die einzelnen Ergebnisse besprochen wurden. Die Besprechungszeit war jedoch dann wiederum zu kurz, sodass wir überziehen mussten und unsere Ergebnisse nur sehr knapp vorstellen konnten.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/seminar/seminar-10/","summary":"Innerhalb des Seminars hat sich unsere Gruppe mit der Gruppierungs-, Einteilung- sowie der Kategorisierungsmöglichkeiten von professionell produzierten Videos beschäftigt. Ebenfalls haben wir betrachtet, wie Metadaten angereichert werden können um zusätzliche Informationen zu codieren. Neben den klassischen Gruppierungsmöglichkeiten nach Genre oder Stilmittel haben wir uns auch damit beschäftigt, wie Filme nach dem Setting oder dem narrativen Stil kategorisiert werden können. Zusätzlich haben wir Einteilungsmöglichkeiten wie Altersfreigabe und Regisseure oder andere Metadaten gefunden.","title":"Seminar 10"},{"content":"  div .quizQuestion { margin: 2rem 0; position: relative; border-radius: .2rem; border: 1px solid var(--border); color: var(--primary); padding: .5rem 1rem .5rem 2rem; } div.quizQuestion p { padding: 0px; display: block; font-size: 1rem; margin-top: 0rem; margin-bottom: 0rem; } div.quizQuestion p:first-child:before { position: absolute; top: -27px; color: var(--primary); font-family: FontAwesome; left: 10px; } div.quizQuestion p:first-child:after { position: absolute; top: -27px; color: var(--primary); left: 2rem; } div.quizQuestion p:first-child:after { color: var(--primary); content: 'Quiz-Frage'; } div.quizQuestion { border-top: 30px solid var(--border); background: var(--theme); color: var(--primary) !important; }  Quizfrage: Wofür stehen die “Five Star Open Data”?\nAntwort: Die Five Star Open Data bilden eine hierarchische Kategorisierungsgruppe nach denen Dateiformate bzw. Technologien hinsichtlich ihrer Güte für eine Verlinkung von Ressourcen kategorisiert werden können.\n  div .answeredQuestion { margin: 2rem 0; position: relative; border-radius: .2rem; border: 1px solid var(--border); color: var(--primary); padding: .5rem 1rem .5rem 2rem; } div.answeredQuestion p { padding: 0px; display: block; font-size: 1rem; margin-top: 0rem; margin-bottom: 0rem; } div.answeredQuestion p:first-child:before { position: absolute; top: -27px; color: var(--primary); font-family: FontAwesome; left: 10px; } div.answeredQuestion p:first-child:after { position: absolute; top: -27px; color: var(--primary); left: 2rem; } div.answeredQuestion p:first-child:after { color: var(--primary); content: 'Beantwortete Frage(n)'; } div.answeredQuestion { border-top: 30px solid var(--border); background: var(--theme); color: var(--primary) !important; }  Frage: Was sind die wichtigsten Merkmale um den Begriff Ontologie einordnen zu können?\nAntwort: Eine Ontologie definiert und beschreibt Ausdrücke, modelliert eine Domäne, basiert auf einem Konsens, ist maschienenlesbar und besitzt eine hohe Aussagekraft bei steigender Komplexität.\n Frage: Was sind RDF-S, OWL und SHACL und wozu kann es jeweils verwendet werden?\nAntwort: RDF-S = RDF-Schema, OWL = Web Ontology Language, SHACL = Shapes Constraint Language RDF Schema ist ein Teil von RDF und wird in RDF beschrieben. Es verfolgt einen objektorientierter Ansatz (Vererbung, Eigenschaften/Methoden). OWL (Full) basiert auf RDF Schema und fügt weitere Beschreibungsmöglichkeiten hinzu (Transitivität, Kardinalität). SHACL ist eine Sprache zur Validierung von RDF-Graphen anhand spezifizierter Bedingungen, die in einem Shape-Modell mit Constraints per RDF beschrieben werden.\n Die Vorlesung behandelte die Thematik Linked Data und Verlinkung von Ressourcen. Dabei habe ich neben den Kernelementen vor allem die fünf Sterne der Open Data kennengelernt, die von Tim Berners-Lee definiert wurden. Ebenso haben wir uns als Kurs dem Thema Content Negotiation genähert.\nBesonders überrascht hat mich, dass proprietäre Dateiformate und Standarddateiformate wie CSV schon unter dem Gesichtspunkt von Linked Data betrachtet werden können. Für mich persönlich war ebenfalls ein Kerngewinn dieser Vorlesung, dass die Vorteile der Verlinkung für die Maschinenenlesbarkeit und Verknüpfung/Aggregation von Daten einen großen Nachteil mit sich zieht: Die Erstellung solcher Informationen/Daten beansprucht deutlich mehr Zeit und ist meist komplexer als die Verwendung von proprietären Dateiformaten oder unstrukturierten Daten.\nMich würde weiterhin interessieren, welche Software es bereits gibt, um die Erstellung solcher verlinkter Daten zu generieren und ob diese für die breite Masse an Internetnutzern benutzerfreundlich ist. Dies wäre aus meiner Sicht ein guter Ausgangspunkt für weitere Recherche oder Projekte. Die Content Negotiation war mir bereits bekannt, leider fand ich die Stelle in der Vorlesung etwas fragwürdig, da wir in vorherigen Beispielen in diesem Modul bereits die Content Negotiation erlebt haben und dies entweder früher einige Fragezeichen aufgeworfen hat oder zum Zeitpunkt dieser Vorlesung Wiederholung war.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/vorlesung/vorlesung-10/","summary":"Die Vorlesung behandelte die Thematik Linked Data und Verlinkung von Ressourcen. Dabei habe ich neben den Kernelementen vor allem die fünf Sterne der Open Data kennengelernt, die von Tim Berners-Lee definiert wurden. Ebenso haben wir uns als Kurs dem Thema Content Negotiation genähert.\nBesonders überrascht hat mich, dass proprietäre Dateiformate und Standarddateiformate wie CSV schon unter dem Gesichtspunkt von Linked Data betrachtet werden können. Für mich persönlich war ebenfalls ein Kerngewinn dieser Vorlesung, dass die Vorteile der Verlinkung für die Maschinenenlesbarkeit und Verknüpfung/Aggregation von Daten einen großen Nachteil mit sich zieht: Die Erstellung solcher Informationen/Daten beansprucht deutlich mehr Zeit und ist meist komplexer als die Verwendung von proprietären Dateiformaten oder unstrukturierten Daten.\nMich würde weiterhin interessieren, welche Software es bereits gibt, um die Erstellung solcher verlinkter Daten zu generieren und ob diese für die breite Masse an Internetnutzern benutzerfreundlich ist. Dies wäre aus meiner Sicht ein guter Ausgangspunkt für weitere Recherche oder Projekte. Die Content Negotiation war mir bereits bekannt, leider fand ich die Stelle in der Vorlesung etwas fragwürdig, da wir in vorherigen Beispielen in diesem Modul bereits die Content Negotiation erlebt haben und dies entweder früher einige Fragezeichen aufgeworfen hat oder zum Zeitpunkt dieser Vorlesung Wiederholung war.","title":"Vorlesung 10"},{"content":"Innerhalb dieses Seminars haben wir uns mit der Verlinkung unseres Medientypens beschäftigt und untersucht, wie Ähnlichkeiten und Beziehungen erkannt und ausgedrückt werden können. Ebenfalls haben wir untersucht, wie aus Verschmelzungen von mehreren medialen Objekte ein Art Kunstform entstehen kann. Dabei haben wir festgestellt, dass die Musik bzw. die einzelnen Bilder eines Videos Gemeinsamkeiten mit anderen Videos aufweisen können (Melodie, Tempo/Pixelwert). Aber auch der Inhalt von verschiedenen Videos kann ähnlich oder gleich sein. Zusätzlich gibt es Algorithmen für die Mustererkennung, Präferenzermittlung und Erfolgsaussichten (siehe Netflix).\nAufgrund der aktuellen technischen Möglichkeiten gibt es jedoch auch verschiedene Kunstformen mit dessen HIlfe der Inhalt durch Deep Fakes verändert werden können. Jedoch bieten auch Parodien, Crossovers, oder Neuverfilmungen die Möglichkeit eine Beziehung/Verlinkung zwischen verschiedenen Medienobjekten herzustellen.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/seminar/seminar-9/","summary":"Innerhalb dieses Seminars haben wir uns mit der Verlinkung unseres Medientypens beschäftigt und untersucht, wie Ähnlichkeiten und Beziehungen erkannt und ausgedrückt werden können. Ebenfalls haben wir untersucht, wie aus Verschmelzungen von mehreren medialen Objekte ein Art Kunstform entstehen kann. Dabei haben wir festgestellt, dass die Musik bzw. die einzelnen Bilder eines Videos Gemeinsamkeiten mit anderen Videos aufweisen können (Melodie, Tempo/Pixelwert). Aber auch der Inhalt von verschiedenen Videos kann ähnlich oder gleich sein.","title":"Seminar 9"},{"content":"  div .quizQuestion { margin: 2rem 0; position: relative; border-radius: .2rem; border: 1px solid var(--border); color: var(--primary); padding: .5rem 1rem .5rem 2rem; } div.quizQuestion p { padding: 0px; display: block; font-size: 1rem; margin-top: 0rem; margin-bottom: 0rem; } div.quizQuestion p:first-child:before { position: absolute; top: -27px; color: var(--primary); font-family: FontAwesome; left: 10px; } div.quizQuestion p:first-child:after { position: absolute; top: -27px; color: var(--primary); left: 2rem; } div.quizQuestion p:first-child:after { color: var(--primary); content: 'Quiz-Frage'; } div.quizQuestion { border-top: 30px solid var(--border); background: var(--theme); color: var(--primary) !important; }  Quizfrage: Welche Möglichkeiten/Sprachen/Formate bietet der RDF-Stack um eine Ontologie zu beschreiben.\nAntwort: Ontologien werden ebenfalls als Triple beschrieben. Dabei bietet RDF Schema, Web Ontology Language (OWL) und die Shape Constraint Language (SHACL) ausdrucksstarke Möglichkeiten eine Ontology zu beschreiben\n  div .answeredQuestion { margin: 2rem 0; position: relative; border-radius: .2rem; border: 1px solid var(--border); color: var(--primary); padding: .5rem 1rem .5rem 2rem; } div.answeredQuestion p { padding: 0px; display: block; font-size: 1rem; margin-top: 0rem; margin-bottom: 0rem; } div.answeredQuestion p:first-child:before { position: absolute; top: -27px; color: var(--primary); font-family: FontAwesome; left: 10px; } div.answeredQuestion p:first-child:after { position: absolute; top: -27px; color: var(--primary); left: 2rem; } div.answeredQuestion p:first-child:after { color: var(--primary); content: 'Beantwortete Frage(n)'; } div.answeredQuestion { border-top: 30px solid var(--border); background: var(--theme); color: var(--primary) !important; }  Frage: Wozu dient Open Refine?\nAntwort: OpenRefine ist ein Programm, mit dessen Hilfe große Datenmengen durchsucht, bereinigt, in andere Formate transformiert und mit anderen Datenmengen verknüpft werden können. Folgende Datenformate werden beispielsweise unterstützt: CSV, XLS, JSON oder XML\nFrage: Was ist die Europeana und in welcher Verbindung steht die DDB zu Europeana?\nAntwort: Europeana ist eine virtuelle Bibliothek, die das wissenschaftliche und kulturelle Erbe Europas in Form von multimedialen Daten zugänglich machen soll.\nDie DDB ist dabei der deutsche Aggregator für Europeana und übermittelt Metadaten und Objekte aus deutschen Kultureinrichtungen. Da die Aufgabe sehr der beiden Institutionen sich sehr ähneln, wird Europeana als die große Schwester von der DDB angesehen.\n Innerhalb dieser Vorlesung haben wir uns dem Thema Ontologie gewidmet und von Herr Dr. Arndt erfahren, wie Ontologien innerhalb des RDF-Stacks beschrieben werden können. Ontologien beschreiben dabei eine formale, explizite Spezifikation einer gemeinsamen Konzeptionierung, die durch eine hohe semantische Ausdruckskraft charakterisiert ist. Durch Ontologien kann man den Graph von Triples in zwei Boxen/Kategorien (T-Box und A-Box) aufteilen. In RDF Schema kann dann mithilfe eines ähnlichen Modells wie in der objekt-orientierten Programmierung eine Hierarchie aufgebaut werden (Vererbung, Eigenschaften/Methoden, etc.). RDF-Container ermöglichen dabei dann eine Reifikation. OWL basiert auf RDF Schema und fügt einige weitere Beschreibungsmöglichkeiten (Transitivität, Kardinalität, etc.) hinzu. SHACL hingegen verfolgt ein anderes Modell, was auf Basis eines Shape-Modells mit Constraints ausgelegt ist.\nAn der Vorlesung hat mir gut gefallen, dass wir weitere Möglichkeiten von RDF in bezug auf die Beschreibung von Ontologien kennengelernt haben. Leider war die Zeit recht knapp, sodass für mich persönlich wichtige Übungen/praktische Anwendungen nicht vermittelt werden konnten. Wünschenswert hier wären praktische Aufgaben in einem nachfolgenden Seminar.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/vorlesung/vorlesung-9/","summary":"Innerhalb dieser Vorlesung haben wir uns dem Thema Ontologie gewidmet und von Herr Dr. Arndt erfahren, wie Ontologien innerhalb des RDF-Stacks beschrieben werden können. Ontologien beschreiben dabei eine formale, explizite Spezifikation einer gemeinsamen Konzeptionierung, die durch eine hohe semantische Ausdruckskraft charakterisiert ist. Durch Ontologien kann man den Graph von Triples in zwei Boxen/Kategorien (T-Box und A-Box) aufteilen. In RDF Schema kann dann mithilfe eines ähnlichen Modells wie in der objekt-orientierten Programmierung eine Hierarchie aufgebaut werden (Vererbung, Eigenschaften/Methoden, etc.). RDF-Container ermöglichen dabei dann eine Reifikation. OWL basiert auf RDF Schema und fügt einige weitere Beschreibungsmöglichkeiten (Transitivität, Kardinalität, etc.) hinzu. SHACL hingegen verfolgt ein anderes Modell, was auf Basis eines Shape-Modells mit Constraints ausgelegt ist.\nAn der Vorlesung hat mir gut gefallen, dass wir weitere Möglichkeiten von RDF in bezug auf die Beschreibung von Ontologien kennengelernt haben. Leider war die Zeit recht knapp, sodass für mich persönlich wichtige Übungen/praktische Anwendungen nicht vermittelt werden konnten. Wünschenswert hier wären praktische Aufgaben in einem nachfolgenden Seminar.","title":"Vorlesung 9"},{"content":"  div .alert { margin: 2rem 0; position: relative; border-radius: .2rem; border: 1px solid #843838; color: #ffffff; padding: .5rem 1rem .5rem 2rem; } div.alert p { padding: 0px; display: block; font-size: 1rem; margin-top: 0rem; margin-bottom: 0rem; } div.alert p:first-child:before { position: absolute; top: -27px; color: #ffffff; font-family: FontAwesome; left: 10px; } div.alert p:first-child:after { position: absolute; top: -27px; color: #ffffff; left: 2rem; } div.alert p:first-child:after { color: #ffffff; content: 'Achtung'; } div.alert { border-top: 30px solid #843838; background: var(--theme); color: var(--primary) !important; }  Aufgrund von Krankheit konnte ich lieder nicht teilnehmen.\n ","permalink":"https://fredericbahr.github.io/lernportfolio/posts/seminar/seminar-8/","summary":"div .alert { margin: 2rem 0; position: relative; border-radius: .2rem; border: 1px solid #843838; color: #ffffff; padding: .5rem 1rem .5rem 2rem; } div.alert p { padding: 0px; display: block; font-size: 1rem; margin-top: 0rem; margin-bottom: 0rem; } div.alert p:first-child:before { position: absolute; top: -27px; color: #ffffff; font-family: FontAwesome; left: 10px; } div.alert p:first-child:after { position: absolute; top: -27px; color: #ffffff; left: 2rem; } div.alert p:first-child:after { color: #ffffff; content: 'Achtung'; } div.","title":"Seminar 8"},{"content":"Exkurs Deutsche Digitale Bibliothek:\nIn dieser Vorlesung hat Frau Bertha einen Gastvortrag über die Deutsche Digitale Bibliothek gehalten. Zentraler Inhalt des Vortrages war die Charakterisierung der DDB, insbesondere als Aggregator für Europeana und die Bereitstellungen von Diensten. Sehr interessant fande ich die Erklärungen zur “unmöglichen Trinität von Metadaten”. Dies war mir vorher nicht so bewusst und hatte meines Erachtens nach einen guten Bezug zum Themengebiet der Vorlesung, da dort ebenfalls Metadaten betrachtet werden. Leider konnte ich dem Vortrag aufgrund der veralteten Darstellung der Präsentation sowie der späten Veranstaltung nur mühselig folgen. Aufgrund dieser Tatsache nehme ich mir für die darauffolgenden Vorlesungen/Seminare vor, konzentrierter und auch aktiver an der Lehrveranstaltung teilzunehmen.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/vorlesung/vorlesung-8/","summary":"Exkurs Deutsche Digitale Bibliothek:\nIn dieser Vorlesung hat Frau Bertha einen Gastvortrag über die Deutsche Digitale Bibliothek gehalten. Zentraler Inhalt des Vortrages war die Charakterisierung der DDB, insbesondere als Aggregator für Europeana und die Bereitstellungen von Diensten. Sehr interessant fande ich die Erklärungen zur “unmöglichen Trinität von Metadaten”. Dies war mir vorher nicht so bewusst und hatte meines Erachtens nach einen guten Bezug zum Themengebiet der Vorlesung, da dort ebenfalls Metadaten betrachtet werden.","title":"Vorlesung 8"},{"content":"Das Seminar hat unsere Arbeitsgruppe dafür verwendet, um Möglichkeiten der Bearbeitung in der Postproduktion von Filmen zu recherchieren. Ebenfalls haben wir uns damit beschäftigt, welche (kommerzielle) professionelle Bearbeitungssoftware für die Postproduktion angeboten wird. Abschließend haben wir zu den typischen Bearbeitungsschritten in der Postproduktion recherchiert und eine graphische Darstellung des Workflows gefunden. Viele der Videoschnittsoftware kannte ich bereits aus eigenen Erfahrungen (Hobby, Studium generale, etc.) und kenne mich mit einer dieser Software (Davinci Resolve) besser aus.\nInteressant war jedoch die Recherche und der Austausch mit den Gruppenmitgliedern über die typischen Bearbeitungsschritte bzw. -reihenfolge. Hier gab es einige kontroverse Meinungen, die sich nach unseren gemeinsamer Nachforschung allesamt als richtig erwiesen haben. Neu für mich war jedoch die Möglichkeiten zur parallelen Verarbeitung in unterschiedlichen Teams.\nEbenso habe ich gelernt, dass auch die Hollywood-Filmproduktion viele verschiedene Subkategorien aufweist und für diese spezielle Tools und Workflows verwendet werden. So entstand die Debatte, ob die Erstellung und Bearbeitung von Cartoons, die komplett digital erstellt werden in die Sparte Manual revision/Authoring passt, da es sich hierbei um keine klassische Bearbeitung, sondern eher um eine Erzeugung handelt.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/seminar/seminar-7/","summary":"Das Seminar hat unsere Arbeitsgruppe dafür verwendet, um Möglichkeiten der Bearbeitung in der Postproduktion von Filmen zu recherchieren. Ebenfalls haben wir uns damit beschäftigt, welche (kommerzielle) professionelle Bearbeitungssoftware für die Postproduktion angeboten wird. Abschließend haben wir zu den typischen Bearbeitungsschritten in der Postproduktion recherchiert und eine graphische Darstellung des Workflows gefunden. Viele der Videoschnittsoftware kannte ich bereits aus eigenen Erfahrungen (Hobby, Studium generale, etc.) und kenne mich mit einer dieser Software (Davinci Resolve) besser aus.","title":"Seminar 7"},{"content":"  div .quizQuestion { margin: 2rem 0; position: relative; border-radius: .2rem; border: 1px solid var(--border); color: var(--primary); padding: .5rem 1rem .5rem 2rem; } div.quizQuestion p { padding: 0px; display: block; font-size: 1rem; margin-top: 0rem; margin-bottom: 0rem; } div.quizQuestion p:first-child:before { position: absolute; top: -27px; color: var(--primary); font-family: FontAwesome; left: 10px; } div.quizQuestion p:first-child:after { position: absolute; top: -27px; color: var(--primary); left: 2rem; } div.quizQuestion p:first-child:after { color: var(--primary); content: 'Quiz-Frage'; } div.quizQuestion { border-top: 30px solid var(--border); background: var(--theme); color: var(--primary) !important; }  Quizfrage: Wozu kann OpenRefine einerseits verwendet werden?\nAntwort: OpenRefine mit der RDF-Extension kann dafür verwendet werden Datensätze einzulesen, zu bearbeiten und zu bereinigen. Zusätzlich können dann die Daten per Mapping in RDF Triples übersetzt werden und so einen RDF-Graphen erstellt werden.\n  div .answeredQuestion { margin: 2rem 0; position: relative; border-radius: .2rem; border: 1px solid var(--border); color: var(--primary); padding: .5rem 1rem .5rem 2rem; } div.answeredQuestion p { padding: 0px; display: block; font-size: 1rem; margin-top: 0rem; margin-bottom: 0rem; } div.answeredQuestion p:first-child:before { position: absolute; top: -27px; color: var(--primary); font-family: FontAwesome; left: 10px; } div.answeredQuestion p:first-child:after { position: absolute; top: -27px; color: var(--primary); left: 2rem; } div.answeredQuestion p:first-child:after { color: var(--primary); content: 'Beantwortete Frage(n)'; } div.answeredQuestion { border-top: 30px solid var(--border); background: var(--theme); color: var(--primary) !important; }  Frage: Was ist DBpedia und warum nutzt man DBpedia?\nAntwort: DBPedia extrahiert Daten aus Wikipedia und stellt diese strukturiert als Linked Data zur Verfügung. Dadurch werden diese Informationen einerseits maschinell abrufbar, andererseits können komplexe Suchanfragen gestellt werden.\n Frage: Nennen Sie eine wesentliche Gemeinsamkeit und einen Unterschied von DBpedia und Wikidata.\nAntwort: Gemeinsamkeiten:\n Versuchen Informationen aus Wikipedia strukturiert zu speichern Abfrage per SPARQL  Unterschiede:\n Aufbau der strukturierten Informationen (RDF vs. JSON, XML, SQL) Repräsentation von Fakten (Triple vs. Statements und Claim) DBpedia extrahiert Daten/Informationen Wikidata erhält Informationen über Eingabe durch Autor   In dieser Vorlesung haben wir uns mit dem Tool OpenRefine beschäftigt, um Daten zu bearbeiten, zu bereinigen oder zu entfernen. Überrascht hat mich die doch komplizierte Installation von OpenRefine unter Windows, wo eine ZIP-Datei gedownloadet und entpackt werden muss und die RDF-Extension im richtigen Ordner, mit richtigem Namen abzulegen war. Hier wäre eine Installation per Wizard oder per CLI wünschenswert. Leider hat dieser Teil auch den Großteil der Vorlesung in Anspruch genommen, sodass wenig Zeit für die praktische Auseinandersetzung mit dem Tool an sich blieb. Jedoch konnte ich das Potenzial von OpenRefine erkennen und war überrascht von der Anzahl an built-in Methoden, um die Daten zu bearbeiten und zu bereinigen. Auch die Verwendung des RDF-Plugins um die Tabelle (Daten) in RDF-Triple umzuwandeln, war hilfreich und verdeutlichte die Einsatzmöglichkeiten dieses Tools in der Praxis. Der mangelnden Zeit schulden will/müsste ich mich aber noch intensiver mit diesem Tool beschäftigen.\nNachtrag: Die weitere Auseinandersetzung hat sich als praktisch erwiesen, da so im Vortrag der essenca GmbH einige parallelen zwischen OpenRefine und essencas Extraktion von Daten und Generierung von RDF Triples erwiesen hat.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/vorlesung/vorlesung-7/","summary":"In dieser Vorlesung haben wir uns mit dem Tool OpenRefine beschäftigt, um Daten zu bearbeiten, zu bereinigen oder zu entfernen. Überrascht hat mich die doch komplizierte Installation von OpenRefine unter Windows, wo eine ZIP-Datei gedownloadet und entpackt werden muss und die RDF-Extension im richtigen Ordner, mit richtigem Namen abzulegen war. Hier wäre eine Installation per Wizard oder per CLI wünschenswert. Leider hat dieser Teil auch den Großteil der Vorlesung in Anspruch genommen, sodass wenig Zeit für die praktische Auseinandersetzung mit dem Tool an sich blieb. Jedoch konnte ich das Potenzial von OpenRefine erkennen und war überrascht von der Anzahl an built-in Methoden, um die Daten zu bearbeiten und zu bereinigen. Auch die Verwendung des RDF-Plugins um die Tabelle (Daten) in RDF-Triple umzuwandeln, war hilfreich und verdeutlichte die Einsatzmöglichkeiten dieses Tools in der Praxis. Der mangelnden Zeit schulden will/müsste ich mich aber noch intensiver mit diesem Tool beschäftigen.\u0026quot;","title":"Vorlesung 7"},{"content":"Innerhalb des Seminars hat sich unsere Gruppe mit den Fragen rund um die verschiedenen Datenformate, die Erfassung und Extraktion von Metadaten sowie spezielle Mediendatenbanken für unser Medium Video beschäftigt. Dabei habe ich mich einerseits mit den verschiedenen generischen Kompressionierungsverfahren für Videos und mit der Oracle Multimedia Datenbank sowie SQL/MM beschäftigt. Erwähnenswert dabei sind die verschiedenen Möglichkeiten ein Video zu komprimieren: Es können jeweils nur die Differenzen zwischen Einzelbildern oder ein Bewegungsvektor für das Verschieben eines Bildelementes innerhalb des Bildes gespeichert werden . Zudem ermöglicht die Entropiekodierung eine derartige Kodierung von Informationen, sodass häufig auftretende Informationen mit möglichst kleinen Codewörtern kodiert werden um Speicherplatz zu sparen.\nEs hat sich zusätzlich herausgestellt, dass mithilfe von Oracle Multimedia verschiedene Metadaten automatisiert beim Einfügen einer Videodatei extrahiert werden und diese mit einer einfachen SQL-Abfrage abgefragt werden können.#\nGut umsetzen konnte ich mein Vorhaben erst genau zu recherchieren bevor ich mein Wissen mit den anderen geteilt habe. Daran möchte ich weiter anknüpfen.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/seminar/seminar-6/","summary":"Innerhalb des Seminars hat sich unsere Gruppe mit den Fragen rund um die verschiedenen Datenformate, die Erfassung und Extraktion von Metadaten sowie spezielle Mediendatenbanken für unser Medium Video beschäftigt. Dabei habe ich mich einerseits mit den verschiedenen generischen Kompressionierungsverfahren für Videos und mit der Oracle Multimedia Datenbank sowie SQL/MM beschäftigt. Erwähnenswert dabei sind die verschiedenen Möglichkeiten ein Video zu komprimieren: Es können jeweils nur die Differenzen zwischen Einzelbildern oder ein Bewegungsvektor für das Verschieben eines Bildelementes innerhalb des Bildes gespeichert werden .","title":"Seminar 6"},{"content":"  div .quizQuestion { margin: 2rem 0; position: relative; border-radius: .2rem; border: 1px solid var(--border); color: var(--primary); padding: .5rem 1rem .5rem 2rem; } div.quizQuestion p { padding: 0px; display: block; font-size: 1rem; margin-top: 0rem; margin-bottom: 0rem; } div.quizQuestion p:first-child:before { position: absolute; top: -27px; color: var(--primary); font-family: FontAwesome; left: 10px; } div.quizQuestion p:first-child:after { position: absolute; top: -27px; color: var(--primary); left: 2rem; } div.quizQuestion p:first-child:after { color: var(--primary); content: 'Quiz-Frage'; } div.quizQuestion { border-top: 30px solid var(--border); background: var(--theme); color: var(--primary) !important; }  Quizfrage: Welchen Vorteil bietet DBpedia gegenüber Wikipedia und wie wird dieser Vorteil erzielt?\nAntwort: DBpedia ermöglicht es, komplexere Suchanfragen zu beantworten und verknüpfte Aussagen über Wissen zu treffen. Dies ist möglich, indem DBpedia semi-strukturelle Informationen aus Wikipedia extrahiert und in RDF-Triples umwandelt, wodurch eine Verknüpfung von Informationen stattfindet.\n  div .answeredQuestion { margin: 2rem 0; position: relative; border-radius: .2rem; border: 1px solid var(--border); color: var(--primary); padding: .5rem 1rem .5rem 2rem; } div.answeredQuestion p { padding: 0px; display: block; font-size: 1rem; margin-top: 0rem; margin-bottom: 0rem; } div.answeredQuestion p:first-child:before { position: absolute; top: -27px; color: var(--primary); font-family: FontAwesome; left: 10px; } div.answeredQuestion p:first-child:after { position: absolute; top: -27px; color: var(--primary); left: 2rem; } div.answeredQuestion p:first-child:after { color: var(--primary); content: 'Beantwortete Frage(n)'; } div.answeredQuestion { border-top: 30px solid var(--border); background: var(--theme); color: var(--primary) !important; }  Frage: Wie ist die Struktur eines Triples in RDF?\nAntwort: Ein Triple besteht immer aus Subjekt, Prädikat und Objekt. Diese werden über URIs bzw. IRIs angegeben. Objekte können zudem entweder leere Knoten oder Literale sein.\n Frage: Wie ist der Aufbau von URIs?\nAntwort: Eine URI besteht aus fünf Bestandteilen: scheme, authority, path, query und fragment. Das häufigste Schema im Web ist dabei http bzw. https.\n Die Vorlesung ging über das Thema DBpedia. Dabei habe ich gelernt, dass es das Ziel von DBpedia ist, die semi-strukturierten Daten von Wikipedia zu extrahieren und in strukturierte Daten zu überführen, um komplexere Suchanfragen zu realisieren. Für mich neu war, dass Wikipedia bereits aus semi-strukturierten Informationen bzw. Daten besteht und ein Grundgerüst für die Artikel vorgibt. Ebenfalls erstaunlich waren die von Herr Dr. Arndt angegebenen Zahlen/Metriken zu der Wissensdatenbank sowie die eigens von DBpedia erstellte Ontologie. Die von DBpedia benutze Pipeline zur Erstellung dieses Wissens setzt dabei auf neue Technologien und Qualitätssicherungen, die mir ebenfalls aus den praktischen Tätigkeiten als Werksstudent geläufig waren, sodass eine Verknüpfung aus Theorie und Praxis zu erkennen war.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/vorlesung/vorlesung-6/","summary":"Die Vorlesung ging über das Thema DBpedia. Dabei habe ich gelernt, dass es das Ziel von DBpedia ist, die semi-strukturierten Daten von Wikipedia zu extrahieren und in strukturierte Daten zu überführen, um komplexere Suchanfragen zu realisieren. Für mich neu war, dass Wikipedia bereits aus semi-strukturierten Informationen bzw. Daten besteht und ein Grundgerüst für die Artikel vorgibt. Ebenfalls erstaunlich waren die von Herr Dr. Arndt angegebenen Zahlen/Metriken zu der Wissensdatenbank sowie die eigens von DBpedia erstellte Ontologie. Die von DBpedia benutze Pipeline zur Erstellung dieses Wissens setzt dabei auf neue Technologien und Qualitätssicherungen, die mir ebenfalls aus den praktischen Tätigkeiten als Werksstudent geläufig waren, sodass eine Verknüpfung aus Theorie und Praxis zu erkennen war.","title":"Vorlesung 6"},{"content":"In diesem Seminar hat uns Herr Dr. Arndt gezeigt, wie mithilfe einer Python-Bibliothek (RDFLib) RDF-Dateien geparset und so eine Graph-Datenstruktur aufgebaut werden kann. Diese Bibliothek kann die erzeugte Graph-Datenstruktur in verschiedene Formate serialisieren und somit persistent auf einem Speichermedium speichern.\nZudem lernte ich, welche Prefixe für URIs existieren und welche Tools zur Erkundung dieser Prefixe geeignet sind. Anschließend erzeugte ich in einer Gruppenarbeit selber einen RDF-Graphen.\nDas gesamte Seminar hat mein Verständnis für die Erstellung von Triples mithilfe von RDF verstärkt.Ebenso half es, die Syntax sowie Semantik des Serialisierungsformates Turtle zu verstehen.. Persönlich gut fande ich die interaktive Gestaltung des Seminars trotz einer rein digitalen Veranstaltung. Insbesondere der Einsatz des Umfragewerkzeuges in Big Blue Button schätzte ich sehr.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/seminar/seminar-5/","summary":"In diesem Seminar hat uns Herr Dr. Arndt gezeigt, wie mithilfe einer Python-Bibliothek (RDFLib) RDF-Dateien geparset und so eine Graph-Datenstruktur aufgebaut werden kann. Diese Bibliothek kann die erzeugte Graph-Datenstruktur in verschiedene Formate serialisieren und somit persistent auf einem Speichermedium speichern.\nZudem lernte ich, welche Prefixe für URIs existieren und welche Tools zur Erkundung dieser Prefixe geeignet sind. Anschließend erzeugte ich in einer Gruppenarbeit selber einen RDF-Graphen.\nDas gesamte Seminar hat mein Verständnis für die Erstellung von Triples mithilfe von RDF verstärkt.","title":"Seminar 5"},{"content":"Vorlesung anstatt Seminar:  div .quizQuestion { margin: 2rem 0; position: relative; border-radius: .2rem; border: 1px solid var(--border); color: var(--primary); padding: .5rem 1rem .5rem 2rem; } div.quizQuestion p { padding: 0px; display: block; font-size: 1rem; margin-top: 0rem; margin-bottom: 0rem; } div.quizQuestion p:first-child:before { position: absolute; top: -27px; color: var(--primary); font-family: FontAwesome; left: 10px; } div.quizQuestion p:first-child:after { position: absolute; top: -27px; color: var(--primary); left: 2rem; } div.quizQuestion p:first-child:after { color: var(--primary); content: 'Quiz-Frage'; } div.quizQuestion { border-top: 30px solid var(--border); background: var(--theme); color: var(--primary) !important; }  Quizfrage: Welche Vorteile bieten die Serialisierungsformate N-Triples, Turtle und RDF/XML?\nAntwort: Das Serialisierungsformat N-Triples legt den Schwerpunkt auf einfaches Parsing, wohingegen das Serialisierungsformat Turtle eine einfachere Lesbarkeit für Menschen realisieren will. Zudem ermöglichen verkürzte Schreibweisen eine Reduktion der Implementierungszeit sowie kleinere Textdateien. RDF/XML ist historisch gewachsen und war das erste Standardserialisierungsformat. Alle drei Formate sind zusätzlich Empfehlungen des W3C.\n\nNach einer kurzen Wiederholung der letzten Vorlesung sowie einer Einordnung des Themas RDF Serialisierung, habe ich zunächst die verschiedenen Serialisierungsformate (N3, Turtle, NTriples, JSON-LD, etc.) sowie deren Mächtigkeiten kennengelernt. Überrascht hat mich dabei die große Anzahl an Serialisierungsformate, die zur Beschreibung von RDF zur Verfügung stehen.\nHilfreich fand ich dabei die bildliche Beschreibung des Prozesses von einem Tripel zum Graphen zur tabellarischen Darstellung. Dies hat mir beim Verstehen der N-Triples/N-Quads aber auch der Turtle Syntax geholfen. Am Ende der Vorlesung habe ich die verschiedenen Syntaxabkürzungen (Komma, Semikolon, Prefix), die Turtle bereitstellt und die Vorteile/Nachteile dieses Serialisierungsformates kennengelernt.\nMein Wunsch und Hoffnungen war es, dass wir uns in einem nachfolgenden Seminar noch praktisch mit dieser Thematik aussetzen. Ausgehend von einem späteren Standpunkt weiß ich, dass diese Hoffnung erfüllt wurde.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/seminar/seminar-4/","summary":"Vorlesung anstatt Seminar:  div .quizQuestion { margin: 2rem 0; position: relative; border-radius: .2rem; border: 1px solid var(--border); color: var(--primary); padding: .5rem 1rem .5rem 2rem; } div.quizQuestion p { padding: 0px; display: block; font-size: 1rem; margin-top: 0rem; margin-bottom: 0rem; } div.quizQuestion p:first-child:before { position: absolute; top: -27px; color: var(--primary); font-family: FontAwesome; left: 10px; } div.quizQuestion p:first-child:after { position: absolute; top: -27px; color: var(--primary); left: 2rem; } div.quizQuestion p:first-child:after { color: var(--primary); content: 'Quiz-Frage'; } div.","title":"Seminar 4"},{"content":"Exkursion Sächsisches Staatsarchiv:\nInnerhalb des Vortrages von Frau Dr. Kluttig habe ich gelernt, dass das Staatsarchiv Leipzig verschiedene Dokumente von diversen Behörden (Reichs-, Bundes, Verwaltungs- und Justizbehörden), aber auch von politischen Parteien, insbesondere der SED, archivieren. Besonders überrascht hat mich die Sicht auf den Lebenszyklus und den Zweck von Dokumenten. Dokumente können dabei einen Primärzweck (der Zweck für den das Dokument erstellt wurde) und einen Sekundärzweck (differenten Zweck nach Primärzweck) aufweisen. Ebenso fängt durch die Archivierung des Dokumentes ein zweiter Lebenszyklus statt.\nErstaunlich war ebenso der bzw. die Prozesse die durchlaufen werden, bis ein Dokument archiviert oder digitalisiert wird. Dabei habe ich gelernt, dass aus Sicht eines Archives die Digitalisierung von Dokumenten nicht immer vorteilhaft ist und Originalquellen präzisere Aussagen/Auswertungen erlauben als die digitalen Kopien.\nDie Einhaltungen von Datenschutzgeschetzen, obwohl die eigentlich betroffene Person bereits verstorben ist, hat mich ebenfalls verwundert. Leider fehlte mir jedoch der genaue Bezug zum Modul “Multimedia Lifecycle Management”, da dieser meines Erachtens nach sehr technisch ausgelegt ist. Frau Dr. Kluttig berichtete größtenteils von analogen Verfahren und Prozessen, die für mich persönlich keinen direkten Bezug zu den Themen der Vorlesungen hatten.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/vorlesung/vorlesung-4/","summary":"Exkursion Sächsisches Staatsarchiv:\nInnerhalb des Vortrages von Frau Dr. Kluttig habe ich gelernt, dass das Staatsarchiv Leipzig verschiedene Dokumente von diversen Behörden (Reichs-, Bundes, Verwaltungs- und Justizbehörden), aber auch von politischen Parteien, insbesondere der SED, archivieren. Besonders überrascht hat mich die Sicht auf den Lebenszyklus und den Zweck von Dokumenten. Dokumente können dabei einen Primärzweck (der Zweck für den das Dokument erstellt wurde) und einen Sekundärzweck (differenten Zweck nach Primärzweck) aufweisen.","title":"Vorlesung 4"},{"content":"Zu Beginn des Seminars wurden die einzelnen Gruppenergebnisse des letzten Seminars kurz vorgestellt. Anschließen haben wir innerhalb einer Diskussionsphase die Fragestellung beantwortet, welche Datenquellen und -sätze für unseren Medientypen Video existieren. Geschätzt habe ich den lockeren Austausch von vorhandenem Wissen mit den Kommilitonen und die Erkenntnis, dass wir uns innerhalb der Bearbeitung auf eine spezielle Einsatzdomäne beschränken wollen.\nGewählt wurde die Domäne der professionellen Videoproduktion. Das Bestreben resultierend aus dem letzten Seminar, erst zu recherchieren und das Wissen zu validieren, konnte ich innerhalb dieses Seminars bereits umsetzen. Die Fragestellung war aus persönlicher Sicht etwas zu detailliert gestellt, sodass nach einer halbstündigen Bearbeitung die Ergebnisse feststanden und die weitere Bearbeitungszeit für mich/uns überflüssig war.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/seminar/seminar-3/","summary":"Zu Beginn des Seminars wurden die einzelnen Gruppenergebnisse des letzten Seminars kurz vorgestellt. Anschließen haben wir innerhalb einer Diskussionsphase die Fragestellung beantwortet, welche Datenquellen und -sätze für unseren Medientypen Video existieren. Geschätzt habe ich den lockeren Austausch von vorhandenem Wissen mit den Kommilitonen und die Erkenntnis, dass wir uns innerhalb der Bearbeitung auf eine spezielle Einsatzdomäne beschränken wollen.\nGewählt wurde die Domäne der professionellen Videoproduktion. Das Bestreben resultierend aus dem letzten Seminar, erst zu recherchieren und das Wissen zu validieren, konnte ich innerhalb dieses Seminars bereits umsetzen.","title":"Seminar 3"},{"content":"  div .quizQuestion { margin: 2rem 0; position: relative; border-radius: .2rem; border: 1px solid var(--border); color: var(--primary); padding: .5rem 1rem .5rem 2rem; } div.quizQuestion p { padding: 0px; display: block; font-size: 1rem; margin-top: 0rem; margin-bottom: 0rem; } div.quizQuestion p:first-child:before { position: absolute; top: -27px; color: var(--primary); font-family: FontAwesome; left: 10px; } div.quizQuestion p:first-child:after { position: absolute; top: -27px; color: var(--primary); left: 2rem; } div.quizQuestion p:first-child:after { color: var(--primary); content: 'Quiz-Frage'; } div.quizQuestion { border-top: 30px solid var(--border); background: var(--theme); color: var(--primary) !important; }  Frage: Wie kann mithilfe von RDF eine Ressource beschrieben werden und welche Eigenschaften besitzt diese Beschreibungsmöglichkeit?\nAntwort: RDF beschreibt mithilfe eines Triples eine Ressource. Das Triples bildet dabei eine logische Aussage durch die Verknüpfung eines Prädikats mit einem Subjekt und Objekt. Triples können als gerichteter Graph dargestellt werden.\n  div .answeredQuestion { margin: 2rem 0; position: relative; border-radius: .2rem; border: 1px solid var(--border); color: var(--primary); padding: .5rem 1rem .5rem 2rem; } div.answeredQuestion p { padding: 0px; display: block; font-size: 1rem; margin-top: 0rem; margin-bottom: 0rem; } div.answeredQuestion p:first-child:before { position: absolute; top: -27px; color: var(--primary); font-family: FontAwesome; left: 10px; } div.answeredQuestion p:first-child:after { position: absolute; top: -27px; color: var(--primary); left: 2rem; } div.answeredQuestion p:first-child:after { color: var(--primary); content: 'Beantwortete Frage(n)'; } div.answeredQuestion { border-top: 30px solid var(--border); background: var(--theme); color: var(--primary) !important; }  Frage: Welchen Nutzen hätte ein umfassendes Semantic Web?\nAntwort: Es bietet durch auffindbare Informationen Transparenz. Informationen liegen maschinenlesbar vor, sodass sie automatisiert integriert werden können. Ebenso fördert es den Austausch von Daten über das Web.\n Frage: Wie unterscheiden sich Web 1.0, Web 2.0 \u0026amp; Web 3.0?\nAntwort: Das Web 1.0 umfasst Dokumente von Institutionen für eine geringe Anzahl an Nutzern, wohingegen das Web 2.0 durch benutzergenerierte Informationen und eine Zentralisierung hin zu großen Services charakterisiert werden kann. Das Web 3.0 schließlich verfolgt eine Dezentralisierung und die Verknüpfung von Informationen (Semantic Web).\n In der Vorlesung habe ich gelernt, wie die einzelnen Fachtermini LOD, Semantic Web, Web of Data und RDF in Verbindung miteinander stehen. Reflektiert betrachtet, erkenne ich, dass mir persönlich Grafiken/Mindmaps enorm helfen um Verbindungen zu erkennen und diese zu verinnerlichen.\nEbenso lernte ich, wie mithilfe des Resource Description Framework (RDF) Ressourcen beschrieben und verlinkt werden können. Dabei werden Triples bestehend aus einem Subjekt, Prädikat und Objekt definiert, die über URIs definiert werden und eine logische Aussage über die Ressource treffen.\nDie Definition von RDF Schemata wurde nur angerissen, sodass noch kein richtiges Verständnis für diese Technologie entstehen konnte. Ich hoffe dies wird in der nächsten Vorlesung weiter behandelt, ansonsten möchte ich etwas Zeit für die Recherche außerhalb der Lehrveranstaltungen in diese Technologie investieren.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/vorlesung/vorlesung-3/","summary":"In der Vorlesung habe ich gelernt, wie die einzelnen Fachtermini LOD, Semantic Web, Web of Data und RDF in Verbindung miteinander stehen. Reflektiert betrachtet, erkenne ich, dass mir persönlich Grafiken/Mindmaps enorm helfen um Verbindungen zu erkennen und diese zu verinnerlichen.\nEbenso lernte ich, wie mithilfe des Resource Description Framework (RDF) Ressourcen beschrieben und verlinkt werden können. Dabei werden Triples bestehend aus einem Subjekt, Prädikat und Objekt definiert, die über URIs definiert werden und eine logische Aussage über die Ressource treffen. Die Definition von RDF Schemata wurde nur angerissen, sodass noch kein richtiges Verständnis für diese Technologie entstehen konnte. Ich hoffe dies wird in der nächsten Vorlesung weiter behandelt, ansonsten möchte ich etwas Zeit für die Recherche außerhalb der Lehrveranstaltungen in diese Technologie investieren.","title":"Vorlesung 3"},{"content":"Innerhalb des Seminars sollten wir in unseren Gruppen verschiedene Lifecycles für unseren Medientypen recherchieren und die wichtigsten Schritte für Institutionen sowie im Linked Data Lifecycle herausarbeiten. Leider konnte ich keine richtigen Lerngewinne verbuchen. Reflektierend hätte sich die Gruppe bzw. ich verstärkt auf eine Recherche konzentrieren sollen, anstatt mit “Halbwissen” die verschiedenen Aufgaben zu diskutieren. Anzumerken sei jedoch, dass die Aufgaben für mich sehr unpräzise und unkonkret formuliert wurden. Ebenso sollten Aussagen über Schritte des Linked Data Lifecycles getroffen werden, die wir bis dato nicht kennengelernt haben.\nDennoch nehme ich mir für die nächsten Seminar vor, erst meine Ideen und bereits bekanntes Wissen zu validieren und diese dann mit meinen Gruppenmitgliedern zu teilen.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/seminar/seminar-2/","summary":"Innerhalb des Seminars sollten wir in unseren Gruppen verschiedene Lifecycles für unseren Medientypen recherchieren und die wichtigsten Schritte für Institutionen sowie im Linked Data Lifecycle herausarbeiten. Leider konnte ich keine richtigen Lerngewinne verbuchen. Reflektierend hätte sich die Gruppe bzw. ich verstärkt auf eine Recherche konzentrieren sollen, anstatt mit “Halbwissen” die verschiedenen Aufgaben zu diskutieren. Anzumerken sei jedoch, dass die Aufgaben für mich sehr unpräzise und unkonkret formuliert wurden. Ebenso sollten Aussagen über Schritte des Linked Data Lifecycles getroffen werden, die wir bis dato nicht kennengelernt haben.","title":"Seminar 2"},{"content":"  div .quizQuestion { margin: 2rem 0; position: relative; border-radius: .2rem; border: 1px solid var(--border); color: var(--primary); padding: .5rem 1rem .5rem 2rem; } div.quizQuestion p { padding: 0px; display: block; font-size: 1rem; margin-top: 0rem; margin-bottom: 0rem; } div.quizQuestion p:first-child:before { position: absolute; top: -27px; color: var(--primary); font-family: FontAwesome; left: 10px; } div.quizQuestion p:first-child:after { position: absolute; top: -27px; color: var(--primary); left: 2rem; } div.quizQuestion p:first-child:after { color: var(--primary); content: 'Quiz-Frage'; } div.quizQuestion { border-top: 30px solid var(--border); background: var(--theme); color: var(--primary) !important; }  Frage: Wofür steht das Akronym RDF und welche Rolle spielt es im Semantic Web.\nAntwort: RDF steht für Resource Description Framework und ist eine grundlegende Technologie des Semantic Webs und beschreibt logische Aussagen über Ressourcen.\n In dieser Vorlesung habe ich gelernt, dass Bibliotheken sich durch die Digitalisierung von reinen analogen Wissenssammlungen für einige Wenige hin zu einer Wissens- und Kommunikationsschnittstelle für die gesamte Gesellschaft entwickelt haben.\nDer größte Lerngewinn bestand im neu erlangten Verständnis für die eigentliche Motivation des Webs und die Ziele des Semantic Webs/Web 3.0. Persönlich würde ich gerne noch mehr über die Verwendung von RDF im Semantic Web und dessen Bedeutung erfahren.\nLeider wurde diese grundlegende Technologie häufig am Anfang der Vorlesung verwendet, ohne dass diese präzise eingeführt wurde. Dies wurde am Ende der Vorlesung in kurzer Zeit versucht nachzuholen, sodass ich nicht nicht alles verstanden habe und hier noch einmal selbst recherchieren muss.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/vorlesung/vorlesung-2/","summary":"In dieser Vorlesung habe ich gelernt, dass Bibliotheken sich durch die Digitalisierung von reinen analogen Wissenssammlungen für einige Wenige hin zu einer Wissens- und Kommunikationsschnittstelle für die gesamte Gesellschaft entwickelt haben.\nDer größte Lerngewinn bestand im neu erlangten Verständnis für die eigentliche Motivation des Webs und die Ziele des Semantic Webs/Web 3.0. Persönlich würde ich gerne noch mehr über die Verwendung von RDF im Semantic Web und dessen Bedeutung erfahren.\nLeider wurde diese grundlegende Technologie häufig am Anfang der Vorlesung verwendet, ohne dass diese präzise eingeführt wurde. Dies wurde am Ende der Vorlesung in kurzer Zeit versucht nachzuholen, sodass ich nicht nicht alles verstanden habe und hier noch einmal selbst recherchieren muss.","title":"Vorlesung 2"},{"content":"Im Seminar habe ich erfahren, dass ich mit meinen Befürchtungen zum Schreiben dieses Lernportfolios nicht alleine bin und konnte ebenso erfahren, dass viele Kommilitoninnen und Kommilitonen die gleichen bzw. ähnliche Erwartungen an die Veranstaltung haben (siehe Vorlesung 13. September).\nDes Weiteren haben wir eine Definition zu Medien erarbeitet: Ein Mittel zum Transport von Informationen, welches eine diskrete oder kontinuierliche Form aufweisen kann. Medien können dabei analog oder digital sein und verschiedene Sinne ansprechen. Ebenso können sie in mehrere Kategorien geclustert werden (siehe Wiki). Reflektierend war ich persönlich überrascht, dass ich noch die Definition von Medien aus dem 2. Semester des Bachelorstudiengangs wusste und diese auch mit den Erwartungen von Herrn Dr. Arndt übereingestimmt hat.\nDie Umfragen und die daraus resultierende Interaktion mit uns Kursteilnehmern empfand ich in beiden Lehrveranstaltungen als sehr angenehm und motivierend. Leider traten immer wieder kleine Wartepausen vor Diskussionen in der “großen Runde” auf, sodass die Veranstaltungen teilweise ermüdend wirkten. Deshalb setze ich mir das Ziel, weiterhin aktiv in den folgenden Vorlesungen/Seminaren zuzuhören und zu interagieren.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/seminar/seminar-1/","summary":"Im Seminar habe ich erfahren, dass ich mit meinen Befürchtungen zum Schreiben dieses Lernportfolios nicht alleine bin und konnte ebenso erfahren, dass viele Kommilitoninnen und Kommilitonen die gleichen bzw. ähnliche Erwartungen an die Veranstaltung haben (siehe Vorlesung 13. September).\nDes Weiteren haben wir eine Definition zu Medien erarbeitet: Ein Mittel zum Transport von Informationen, welches eine diskrete oder kontinuierliche Form aufweisen kann. Medien können dabei analog oder digital sein und verschiedene Sinne ansprechen.","title":"Seminar 1"},{"content":"Ich habe gelernt, wie die Methode \u0026ldquo;Constructive Alignment\u0026rdquo; aufgebaut ist und angewendet wird. Dabei hat mich überrascht, dass Herr Dr. Arndt seine Methode der Vorlesung vorstellt und diese eingehend erläutert. Dies ist für mich neu und hat mich im positiven Maße überrascht, da so didaktische Entscheidungen und der Aufbau der Vorlesung nachvollziehbarer erscheinen.\nEbenso wurde innerhalb der Vorlesung die wesentlichen Schritte des Lebenszyklus von Daten, insbesondere von medialen Daten, erklärt und besprochen, was mir persönlich einen ersten Einblick in das Management eines Lebenszyklusses ermöglicht hat.\nIch erwarte bzw. erhoffe mir von der Veranstaltung Media Lifecycle Management zunächst einen Einblick in die verschiedenen Medienarten und ihre Besonderheiten. Ebenso möchte ich gerne den gesamten Lebenszyklus näher kennenlernen und die Merkmale eines jeweiligen Schrittes vermittelt bekommen. Darüber hinaus würde ich gerne nähere Informationen zur praktischen Relevanz und Umsetzung erhalten. Außerdem wäre die dedizierte Erwähnung von Herausforderungen bzw. Besonderheiten der jeweiligen Medienarten für ein besseres Verständnis wünschenswert.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/vorlesung/vorlesung-1/","summary":"Ich habe gelernt, wie die Methode \u0026ldquo;Constructive Alignment\u0026rdquo; aufgebaut ist und angewendet wird. Dabei hat mich überrascht, dass Herr Dr. Arndt seine Methode der Vorlesung vorstellt und diese eingehend erläutert. Dies ist für mich neu und hat mich im positiven Maße überrascht, da so didaktische Entscheidungen und der Aufbau der Vorlesung nachvollziehbarer erscheinen.\nEbenso wurde innerhalb der Vorlesung die wesentlichen Schritte des Lebenszyklus von Daten, insbesondere von medialen Daten, erklärt und besprochen, was mir persönlich einen ersten Einblick in das Management eines Lebenszyklusses ermöglicht hat.","title":"Vorlesung 1"}]